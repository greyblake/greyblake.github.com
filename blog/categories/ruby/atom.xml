<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ruby | Sergey Potapov]]></title>
  <link href="http://greyblake.com/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://greyblake.com/"/>
  <updated>2018-02-18T19:03:19+01:00</updated>
  <id>http://greyblake.com/</id>
  <author>
    <name><![CDATA[Sergey Potapov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NLP, Toki Pona and Ruby. Part 2: language detector]]></title>
    <link href="http://greyblake.com/blog/2015/09/25/nlp-toki-pona-and-ruby-par2-language-detector/"/>
    <updated>2015-09-25T23:55:00+02:00</updated>
    <id>http://greyblake.com/blog/2015/09/25/nlp-toki-pona-and-ruby-par2-language-detector</id>
    <content type="html"><![CDATA[<p>Previous articles:</p>

<ul>
<li><a href="/blog/2015/09/20/nlp-toki-pona-and-ruby-part1">Part 1: Intro. Tokenizer</a></li>
</ul>


<p>In the first article we created a simple tokenizer, today we're going to create a
language detector to identify Toki Pona text among other texts.</p>

<p>First I want to say that are at least few good libraries for detecting natural languages in ruby:</p>

<ul>
<li><a href="https://github.com/feedbackmine/language_detector">language_detector</a> - detector based on <a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a></li>
<li><a href="https://github.com/peterc/whatlanguage">whatlanguage</a> - detector based on <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a></li>
</ul>


<p>But those are for mainstream: French, English, German... We want Toki Pona!
Also, since we are focused on Toki Pona only, we can get much more precise results.</p>

<!--more-->


<h2>The algorithm</h2>

<p>My initial idea was dead simple: since there are only 125 Toki Pona words, we can
simply check whether a token is a Toki Pona word or something else. Then it's
easy to calculate a density of Toki Pona words in a given phrase and compare it
against some threshold, where 0 &lt; threshold &lt;= 1.</p>

<p>Here is an example of how such algorithm would work with threshold=0.75 and given
phrase <em>"mi moka e kala suli"</em>.</p>

<p>```
Phrase:   mi   moka   e    kala    suli
Weights:  1    0      1    1       1</p>

<p>Sum weight: 1 + 0 + 1 + 1 + 1 = 4
Words count: 5
Density: 4 / 5 = 0.8</p>

<p>0.8 > threshold => true (it's Toki Pona)
```</p>

<p>The phrase has a typo: instead of word <code>moka</code> there should be word <code>moku</code>.
<em>"mi moku e kala suli"</em> means "I am eating a big fish".</p>

<p>I decided to make the algorithm face real data: I picked ~100 random message
from #tokipona IRC channels and split them into three groups:</p>

<ol>
<li>Messages in Toki Pona</li>
<li>Messages in other languages (mostly English)</li>
<li>Mixed messages (half Toki Pona and half English)</li>
</ol>


<p>I wrote a spec, I expected <code>LanguageDetector.toki_pona?(text)</code> to return <code>true</code> for the first group of messages
and <code>false</code> for others. After playing with the value of the threshold, I made it
work almost for all cases.</p>

<p>One of messages looked like this:
<code>
Moku pona xD
</code></p>

<p>It's obvious that it's pure Toki Pona, then what's wrong with it?
The issue was in the <code>Tokenizer</code> that we had implemented in the <a href="/blog/2015/09/20/nlp-toki-pona-and-ruby-part1">previous article</a>.</p>

<p>For this message it returns 4 tokens: <code>["Moku", "pona", "x", "D"]</code>. While
<code>Moku</code> and <code>pona</code> belong to Toki Pona  vocabulary,  <code>x</code> and <code>D</code> don't.</p>

<h2>Updating Tokenizer</h2>

<p>We, as humans, can see that <code>xD</code> actually must be one token, and it's not a regular word, but a smile.
So I had to update <code>Tokenizer</code> to distinguish words and smiles.</p>

<p>That's the point where I had to introduce the difference between <strong>tokens</strong> and <strong>lexemes</strong>.</p>

<p>As the famous <a href="https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools">Dragon Book</a> says about lexemes:</p>

<p><blockquote><p>A lexeme is a sequence of characters in the source program that matches the pattern<br/>for a token and is identified by the lexical analyzer as an instance of that token.</p></blockquote></p>

<p>And about tokens:</p>

<p><blockquote><p>A token is a pair consisting of a token name and an optional attribute value.<br/>The token name is an abstract symbol representing a kind of lexical unit, e.g.,<br/>a particular keyword, or sequence of input characters denoting an identifier.<br/>The token names are the input symbols that the parser processes.</p></blockquote></p>

<p>So I've updated <code>Tokenizer</code> to return array of hashes with <code>:lexeme</code> and <code>:type</code> keys.
By now I have 3 types of tokens: <em>word</em>, <em>smile</em> and <em>punctuation</em>.</p>

<p>Example:
```ruby
Tokenizer.tokenize "moku li ike :("  # Translation: food is bad</p>

<h1>=> [ {:lexeme=>"moku", :type=>:word}, {:lexeme=>"li", :type=>:word},</h1>

<h1>{:lexeme=>"ike", :type=>:word}, {:lexeme=>":/", :type=>:smile}]</h1>

<p>```</p>

<p>The implementation of <code>Tokenizer</code> now is the following:</p>

<p>```ruby
class Tokenizer
  SMILE_REGEXP = /</p>

<pre><code>(?:
  (?: : | ; | = )                            # eyes
  -?                                         # nose
  (?: \) | \| | \\ | \/ | D | P | p | \* )   # mouth
) | (?:x|X)D                                 # other (e.g. XD)
</code></pre>

<p>  /x
  WORD_REGEXP = /\w+/
  PUNCTUATION_REGEX = /[<sup>\s]*/</sup>
  LEXEME_REGEXP = /#{SMILE_REGEXP}|#{WORD_REGEXP}|#{PUNCTUATION_REGEX}/</p>

<p>  def self.tokenize(text)</p>

<pre><code>new(text).tokenize
</code></pre>

<p>  end</p>

<p>  def initialize(text)</p>

<pre><code>@text = text
</code></pre>

<p>  end</p>

<p>  def tokenize</p>

<pre><code>lexemes = @text.scan(LEXEME_REGEXP)
lexemes.map { |lex| lexeme_to_token(lex) }
</code></pre>

<p>  end</p>

<p>  private def lexeme_to_token(lexeme)</p>

<pre><code>{ lexeme: lexeme, type: token_type(lexeme) }
</code></pre>

<p>  end</p>

<p>  private def token_type(lexeme)</p>

<pre><code>case lexeme
when SMILE_REGEXP then :smile
when WORD_REGEXP  then :word
else :punctuation
end
</code></pre>

<p>  end
end
```</p>

<p>Now tokenization process contains two stage: detecting lexemes and defining tokens
for those lexemes. Note, that the order of regular expressions in <code>token_type(lexeme)</code>
method is important. Since <code>SMILE_REGEXP</code> and <code>WORD_REGEXP</code> can overlap (e.g. "XD"),
we want <code>SMILE_REGEXP</code> to have higher priority. The same with punctuation: everything
what is not a smile or a word we consider as punctuation.</p>

<h2>Levenshtein distance</h2>

<p>So I've I updated the language detection algorithm to count only words. Still
there are number of things to improve.</p>

<p>What if some words are meant to be Toki Pona words but they contain a typo?
Actually we can detect them and adjust the algorithm to count them as well.</p>

<p>So to detect a word with a typo we need somehow to calculate word similarity.
And actually what we need is called <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a>.</p>

<p><blockquote><p>Levenshtein distance between two words is the minimum number of single-character<br/>edits (i.e. insertions, deletions or substitutions) required to change one word into the other.</p></blockquote></p>

<p>Levenshtein distance is used in computer science (e.g. for spell checkers), genetics (comparison of gens)
and likely in some other areas. The algorithm isn't the simplest one, but is not hard to understand, so
I encourage you to take a look at <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">wikipedia</a> to
uderstand it better.</p>

<p>Now back to our case: how do we measure the difference between words <code>moka</code> and <code>moku</code>?
The levenshtein distance for them is 1, because only one edit needs to be performed to make <code>moka</code> become
<code>moku</code>: replace <code>a</code> with <code>u</code>.</p>

<p>We can implement it based on the algorithm ourself (actually I did it for fun, but then replaced it
with <a href="https://github.com/threedaymonk/text/blob/master/lib/text/levenshtein.rb">this implementation</a>) or google
for existing solutions.
One of suprises was to find it in
<a href="https://github.com/rubygems/rubygems/blob/45966be372d85520630143090b82b455d287cec6/lib/rubygems/text.rb#L42-L72">rubygems</a> gem.
I guess it's used when one mistypes name of gem in order to have "Did you mean ...?" feature.</p>

<h2>Updating the detection algorithm</h2>

<p>Now, we can adjust the LanguageDetector to be not so strict with typos, and give a word score <code>0.5</code> if it has
levenshtein distance 1 with one of Toki Pona words. Considering the previous example with phrase <em>"mi moka e kala suli"</em>,
the density will be 0.9:</p>

<p>```
Phrase:   mi   moka   e    kala    suli
Weights:  1    0.5    1    1       1</p>

<p>Sum weight: 1 + 0.5 + 1 + 1 + 1 = 4.5
Words count: 5
Density: 4.5 / 5 = 0.9</p>

<p>0.9 > threshold => true (it's Toki Pona)
```</p>

<p>In reality, apart from the mentioned 125 words, the Toki Pona vocablurary includes names of languages, countries and cities.
So the real detector is slightly more complected. You can check it
<a href="https://github.com/greyblake/tokipona/blob/60d8ec72f2da6af26440239e8cb1f0fed5bea8a5/lib/tokipona/language_detector.rb">here</a>.</p>

<p>Btw, the entire implementation of what is being described can be found as the project at
github <a href="https://github.com/greyblake/tokipona">greyblake/tokipona</a>.</p>

<p>That's it for now! In the next part we will try to implement grammar and parser for Toki Pona!</p>

<h2>Links</h2>

<ul>
<li><a href="blog/2015/09/20/nlp-toki-pona-and-ruby-part1">Part 1: implementing Tokenizer</a></li>
<li><a href="http://tokipona.org/">Official Toki Pona site</a></li>
<li><a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance at Wikipedia</a></li>
<li><a href="https://github.com/greyblake/tokipona">My tokipona project at Github</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLP, Toki Pona and Ruby: part 1]]></title>
    <link href="http://greyblake.com/blog/2015/09/20/nlp-toki-pona-and-ruby-part1/"/>
    <updated>2015-09-20T20:36:00+02:00</updated>
    <id>http://greyblake.com/blog/2015/09/20/nlp-toki-pona-and-ruby-part1</id>
    <content type="html"><![CDATA[<h2>Intro</h2>

<p>During last few years, I spent a lot of time learning foreign languages like Esperanto, Spanish and German.
After a while, I came up with an idea that I can apply this knowledge in computer science.</p>

<p>When I decided this I was completely new to Computational Linguistics(CL) and Natural Language Processing(NLP).
However after reading a number of articles I got some basic ideas.</p>

<h2>What I am gonna do</h2>

<p>To dive into CL/NLP I've decided implement Toki Pona -> English translator from scratch.
It's interesting to see which issues I will face and how I will solve them.
It will make me go through number of stages of language processing:</p>

<ul>
<li>Lexical analysis</li>
<li>Language detection (I want to distinguish Toki Pona from other languages)</li>
<li>Morphological analysis (actually will be skipped because of simplicity of Toki Pona)</li>
<li>Syntax analysis</li>
<li>Word translation</li>
<li>Syntax tree conversion</li>
<li>Generation of final translation with respect to English grammar.</li>
</ul>


<p>Anyway, this list is not strict, and probably it will be modified in the future.</p>

<h2>What I am not gonna do</h2>

<p>There are many tools and libraries that already exist in Ruby for NLP.
I am not gonna use any of them here neither cover them in the articles.
If you need something like that, please take a look at <a href="https://github.com/diasks2/ruby-nlp">ruby-nlp</a>.
It's a document that gathers a variety of NLP tools implemented in ruby.</p>

<!--more-->


<h2>What is Toki Pona?</h2>

<p><a href="https://en.wikipedia.org/wiki/Toki_Pona">Toki Pona</a> is a constructed language created by Sonja Lang in 2001.
What is so special about it? Its vocabulary is limited and contains only <strong>125 words</strong>.
The grammar is regular (anyway there will be some pitfalls). The language itself simple and can be learned in 1-2 nights,
and I believe it allows to express 80-90% of daily human communication. Also, it has some philosophical background:
speaking the language you realize what things really are.</p>

<p>Example: there is no word like "friend", one would say "jan pona", what literally  means "good person/human".
In similar way "an ocean" is "telo suli" (big water), "juice" is "telo kili" (water of fruit or vegetable), etc.</p>

<p>So, even Toki Pona is not real <em>natural</em> language, it's good to experiment with, and it gives me some hope that my
goal can be achieved :)</p>

<p>And the end of this article you'll find number of useful links if you want to get into the language.</p>

<h2>First step: lexical analysis</h2>

<p>The first step in processing natural or programming language is <strong>lexical analysis</strong>. It means splitting sequence of
characters into some meaningful units: <strong>tokens</strong>. Sometimes the process is called <strong>tokenization</strong> and
the tools that do it are <strong>tokenizers</strong> or <strong>lexical analyzers</strong>.</p>

<p>Let's see an example. Given a sentence:
<code>
jan suli li pona.
</code>
Translation: "Big man is good"
(<em>jan</em> - human/man, <em>suli</em> - big, <em>li</em> - is/are, <em>pona</em> - good).</p>

<p>Note: in Toki Pona the main word goes first, so noun(<em>jan</em>) is on the first position,
and on the second position is adjective(<em>suli</em>) that modifies the noun.</p>

<p>Expected list of tokens is
<code>ruby
["jan", "suli", "li", "pona", "."]
</code></p>

<p>Let's implement class <code>Tokipona::Tokenizer</code> with a class method <code>.tokenize</code> that returns an array of
tokens for a given text. We start with tests first.</p>

<p>```ruby
describe Tokipona::Tokenizer do
  describe ".tokenize" do</p>

<pre><code>context "only words" do
  it "returns array of words" do
    text = "toki mi li toki pona"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["toki", "mi", "li", "toki", "pona"]
  end
end

context "words with multiple spaces in between" do
  it "returns array of words" do
    text = "toki   mi   li   toki   pona"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["toki", "mi", "li", "toki", "pona"]
  end
end

context "words with special characters" do
  it "returns array of words and characters" do
    text = "sina wile lape anu seme, jan lane?"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["sina", "wile", "lape", "anu", "seme", ",", "jan", "lane", "?"]
  end
end

it "does not change input text" do
  text = "toki mi li pona"
  described_class.tokenize(text)
  expect(text).to eq "toki mi li pona"
end
</code></pre>

<p>  end
end
```</p>

<p>Usually lexical analysis for programming languages is based on <a href="http://web.cse.ohio-state.edu/~gurari/course/cse756/html/cse756se2.html">finite-state automata</a>.
But in our simple case we can easily handle it with one regular expression:</p>

<p>```ruby
module Tokipona
  class Tokenizer</p>

<pre><code>def self.tokenize(text)
  @text.scan(/\w+|[^\s]/)
end
</code></pre>

<p>  end
end
```</p>

<p>This implementation looks very naive, but specs pass, so we leave it as it is.
Probably in the future we will modify.</p>

<h2>Conclusion</h2>

<p>It is the first article and the beginning of the journey. The next step will be an implementation
of Toki Pona language detector. It's not necessary to know Toki Pona to follow me,
but in case you are interested, here below I provide some useful links, so you can learn yourself
and start communicating.</p>

<p>I've created a github repository where you can access the code: <a href="https://github.com/greyblake/tokipona">greyblake/tokipona</a>.</p>

<p>P.S.</p>

<p>Thanks for reading. The subject is new for me, so your comments, suggestions and feedback can be very helpful.</p>

<h2>Links</h2>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Toki_Pona">Toki Pona at Wikipedia</a></li>
<li><a href="http://tokipona.org/">Official Toki Pona site</a></li>
<li><a href="http://rowa.giso.de/languages/toki-pona/english/lessons.php">Toki Pona lessons</a> - here you can start learning the language</li>
<li><a href="http://x-raizor.github.io/visual-tokipona/index.html">Toki Pona visual vocabulary</a></li>
<li>#tokipona - IRC channel where you can communicate with other people</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lazy object pattern in ruby]]></title>
    <link href="http://greyblake.com/blog/2014/10/05/lazy-object-pattern-in-ruby/"/>
    <updated>2014-10-05T21:27:00+02:00</updated>
    <id>http://greyblake.com/blog/2014/10/05/lazy-object-pattern-in-ruby</id>
    <content type="html"><![CDATA[<p>I few days ago my colleague <a href="https://github.com/albertosaurus">Arthur Shagall</a> reviewing my
code suggested me to use <strong>Lazy Object</strong> pattern to postpone some calculations during the load time.
I hadn't heard about the pattern before and even googling it didn't give my much information.
So I have decided to write this article to cover the topic.</p>

<h2>Intention</h2>

<p><strong>Lazy Object</strong> allows you to postpone some calculation until the moment when the actual
result of the calculation is used. That may help you to speed up booting of the application.</p>

<h2>Implementation</h2>

<p>It is pretty simple. We create a proxy object that takes a calculation
block as its property and execute it on first method call.</p>

<p>```ruby
class LazyObject &lt; ::BasicObject
  def initialize(&amp;callable)</p>

<pre><code>@callable = callable
</code></pre>

<p>  end</p>

<p>  def <strong>target_object</strong></p>

<pre><code>@__target_object__ ||= @callable.call
</code></pre>

<p>  end</p>

<p>  def method_missing(method_name, *args, &amp;block)</p>

<pre><code>__target_object__.send(method_name, *args, &amp;block)
</code></pre>

<p>  end
end
```</p>

<h2>Usage example 1</h2>

<p>A constant assignment like this:
<code>ruby
SQUARES = Array.new(10) { |i| i** 2}
</code></p>

<p>Could be converted to this one:</p>

<p><code>ruby
SQUARES = LazyObject.new { Array.new(10) { |i| i** 2} }
</code></p>

<p>So now if you want to use <code>SQUARES</code> it still behaves like an array:</p>

<p><code>ruby
SQUARES.class  # =&gt; Array
SQUARES.size   # =&gt; 10
SQUARES        # =&gt; [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
</code></p>

<h2>Usage example 2</h2>

<p>Let's say you have models <code>State</code> and <code>Address</code> in you Rails application.
What you want do is to validate inclusion of <code>address.state</code> in states.</p>

<p>You can just hardcore the list of states:</p>

<p>```ruby
class Address &lt; ::ActiveRecord::Base
  STATES = ["AL", "AK", "AZ", "AR", "CA", "CO"]   # and so on</p>

<p>  validates :state, inclusion: { in: STATES }
end
```
But it does not reflect your changes in DB in any way.</p>

<p>Then you can fetch the values from DB:</p>

<p><code>ruby
STATES = State.all.map(&amp;:code)
</code></p>

<p>It seems to look better, but there are 2 possible pitfalls:</p>

<ul>
<li>It increases load time (1 more SQL query)</li>
<li>It may cause real troubles if <code>STATES</code> is initialized before <code>State</code> model is seeded. In this case <code>STATES</code> will be empty.</li>
</ul>


<p>So that is the situation where <strong>Lazy Object</strong> is useful:</p>

<p><code>ruby
STATES = LazyObject.new { State.all.map(&amp;:code) }
</code></p>

<h2>Ruby gem</h2>

<p>If your prefer to have it as a ruby gem,
please take a look at <a href="http://rubygems.org/gems/lazy_object">rubygems.org/gems/lazy_object</a>.</p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to compare audio in ruby]]></title>
    <link href="http://greyblake.com/blog/2013/12/19/how-to-compare-audio-in-ruby/"/>
    <updated>2013-12-19T19:51:00+01:00</updated>
    <id>http://greyblake.com/blog/2013/12/19/how-to-compare-audio-in-ruby</id>
    <content type="html"><![CDATA[<h2>Or how to implement sound_like RSpec matcher</h2>

<p>The problem I'm trying to solve in this article is comparison of two
audio files. We'll figure out how to verify that they sound similar.</p>

<p>I was developing an application that has a deal with audio processing and
I had to write a test to verify outcome audio file matches a one
from fixtures. Well, I've decided to compare audio binaries like these:</p>

<p><code>ruby
expect(File.read('outcome.mp3')).to eq File.read('fixture.mp3')
</code></p>

<p>And it worked!</p>

<p>But soon my colleagues let me know I had broken the build. It turned out
that <code>outcome.mp3</code>generated on their Mac books didn't match <code>fixture.mp3</code>
generated on my linux laptop, despite the fact that both sounded
absolutely the same. Probably we had different codecs.
So I had to come up with a better idea.</p>

<!--more-->


<h2>Audio fingerprints and Chromaprint</h2>

<p>After some investigation I found a term "audio fingerprint" or "acoustic fingerprint",
it was exactly what I was looking for. From Wikipedia:</p>

<blockquote><p>An acoustic fingerprint is a condensed digital summary, deterministically generated
from an audio signal, that can be used to identify an audio sample or quickly locate
similar items in an audio database</p></blockquote>

<p>It's used by services like Shazam to identify songs.</p>

<p>So I started looking for open source implementations and found
<a href="http://acoustid.org/chromaprint">Chromaprint</a> - a C library that calculates audio fingerprints
from raw audio files. It seemed to be simple, with good source documentation
and easy to get started.</p>

<h2>Integrate Chromaprint with Ruby</h2>

<p>I found no already existing bindings, so I've implemented
<a href="https://github.com/TMXCredit/chromaprint">my own</a>. Instead of using C,
I gave <a href="https://github.com/ffi/ffi">FFI</a> a shot and it worked perfect!
As result I had stuff that worked the following way:</p>

<p><code>ruby
context     = Chromaprint::Context.new(44100, 1)
fingerprint = context.get_fingerprint(raw_audio_data)
fingerprint.raw # =&gt; [294890785, 328373552, 315802880, 303481088, ...]
</code></p>

<p>According to Chromaprint's documentation a raw fingerprint is an array of 4 byte integers.
But how to compare to 2 fingerprints to detect similarity?</p>

<h2>Hamming distance</h2>

<p>The answer was to calculate Hamming distance from binary representation of fingerprints.
Again according to Wikipedia:</p>

<blockquote><p>In information theory, the Hamming distance between two strings of equal length is
the number of positions at which the corresponding symbols are different. In another
way, it measures the minimum number of substitutions required to change one string
into the other, or the minimum number of errors that could have transformed one
string into the other.</p></blockquote>

<p>To calculate Hamming distance for binary data we need to apply XOR operation and count
number of 1 in the result.</p>

<p>Here is a small example for 2 byte values:</p>

<pre><code>dec     bin
11737   00101101 11011001
27129   01101001 11111001

XOR     01000100 00100000

Hamming distance is 3
</code></pre>

<p>Basing on this I implemented an additional method <code>Fingerprint#compare(fingerprint)</code>
that calculates similarity in range from 0 to 1.</p>

<h2>Create RSpec matcher</h2>

<p>Now I could compare raw audio data, but in real world almost always we have to have a deal
with compressed audio like mp3 or ogg. However wav files contain exactly raw audio data.
So I could convert compressed audio to wav, then read it to get raw audio and
calculate fingerprints for comparison. To convert audio I prefer using <code>sox</code>
command line tool, it's pretty powerful.</p>

<p>I have to explain that I did it all to avoid having a deal with
audio codecs within ruby, since it would make things be much more complicated.</p>

<p>Finally I got <code>sound_like</code> RSpec matcher:</p>

<p>```ruby</p>

<h1>Compare sound of two audio files.</h1>

<h1>Based on the Chromaprint library and the +sox+ command like tool.</h1>

<p>#</p>

<h1>@example</h1>

<h1>"/Airborne.mp3".should sound_like "/ACDC.mp3"</h1>

<h1>"/Children_of_Bodom.mp3".should_not sound_like "/Britney_Spears.mp3"</h1>

<p>RSpec::Matchers.define :sound_like do |expected_file|
  match do |file|</p>

<pre><code>rate      = 96000
channels  = 1
threshold = 0.95

if File.exists?(expected_file) &amp;&amp; File.exists?(file)
  # Convert input files into raw 16-bit signed audio (WAV) to
  # process with Chromaprint:
  sox_command    = "sox %s -e signed -b 16 -t wav - " \
                   "rate #{rate} channels #{channels} 2&gt; /dev/null"
  expected_audio = %x"#{sox_command % [expected_file]}"
  audio          = %x"#{sox_command % [file]}"

  # Get audio fingerprints:
  chromaprint = Chromaprint::Context.new(rate, channels)
  expected_fp = chromaprint.get_fingerprint(expected_audio)
  fp          = chromaprint.get_fingerprint(audio)

  # Compare fingerprints and compare result against threshold:
  expected_fp.compare(fp) &gt; threshold
else
  false
end
</code></pre>

<p>  end
end
```</p>

<p>Note that I used threshold with value 0.95 because quite rare fingerprints
have 100% match.</p>

<h2>Links</h2>

<ul>
<li><a href="https://github.com/TMXCredit/chromaprint">Chromaprint ruby port on github</a></li>
<li><a href="http://acoustid.org/chromaprint">Chromaprint web page</a></li>
<li><a href="http://en.wikipedia.org/wiki/Acoustic_fingerprint">Acoustic fingerprint in Wikipedia</a></li>
<li><a href="http://en.wikipedia.org/wiki/Hamming_distance">Hamming distance in Wikipedia</a></li>
<li><a href="http://stackoverflow.com/a/6397116/1013173">Most efficient way to calculate Hamming distance in ruby</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to call bash(not shell) from ruby]]></title>
    <link href="http://greyblake.com/blog/2013/09/21/how-to-call-bash-not-shell-from-ruby/"/>
    <updated>2013-09-21T21:53:00+02:00</updated>
    <id>http://greyblake.com/blog/2013/09/21/how-to-call-bash-not-shell-from-ruby</id>
    <content type="html"><![CDATA[<p>Few days ago I was writing a ruby wrapper for <a href="http://sox.sourceforge.net/">SoX</a>
command line tool. To reduce disk IO I wanted to use <a href="http://en.wikipedia.org/wiki/Process_substitution">process substitution</a>.
It's a cool shell feature which allows to use command output as an input file for another command.
It's pretty useful if the second command doesn't work with standard input or you need
to pass more than 1 input.</p>

<p>Let me show the classic example(works in bash and zsh):</p>

<p>```bash
cat &lt;(echo 'Saluton!') &lt;(echo 'Kiel vi fartas?')</p>

<h1>=> Saluton! Kiel vi fartas?</h1>

<p>```</p>

<p>So statement <code>&lt;(echo 'Saluton!')</code> is treated like a file which contains line <code>Saluton!</code>.
Underhood bash(zsh) creates a named pipeline where output of <code>echo 'Saluton!'</code> is written.
Then the named pipeline is passed to <code>cat</code> command.</p>

<p>You can see it:</p>

<p>```bash
echo  &lt;(echo 'Saluton!')</p>

<h1>=> /dev/fd/63</h1>

<p>```</p>

<p>So I wanted to use it in ruby:
<code>ruby
cmd = "cat &lt;(echo 'Saluton!') &lt;(echo 'Kiel vi fartas?')"
system(cmd)
</code></p>

<p>But unfortunately it doesn't work:
<code>
sh: 1: Syntax error: "(" unexpected
</code></p>

<p>The problem is that ruby's <code>system</code> method and back quotes use<code>sh</code>
not your current shell (which in my case is <code>bash</code>).</p>

<p>```ruby
system "echo $0"</p>

<h1>=> sh</h1>

<p>```</p>

<p>In shells <code>$0</code>points to the current script or to interpreter if you're running it interactively.</p>

<p>Fortunately there is a way to create a workaround to run bash:</p>

<p>```ruby
require 'shellwords'</p>

<p>def bash(command)
  escaped_command = Shellwords.escape(command)
  system "bash -c #{escaped_command}"
end
```</p>

<p>Bash has option <code>-c</code> which takes bash script to execute.
<a href="http://www.ruby-doc.org/stdlib-2.0/libdoc/shellwords/rdoc/Shellwords.html">Shellwords</a>
is a standard ruby library which provides a method to escape shell commands.</p>

<p>So now it works as we want it to be:</p>

<p><code>ruby
bash("echo $0")  # =&gt; bash
cmd = "cat &lt;(echo 'Saluton!') &lt;(echo 'Kiel vi fartas?')"
bash(cmd)        # =&gt; Saluton! Kiel vi fartas?
</code></p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
</feed>

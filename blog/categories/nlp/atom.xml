<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: nlp | Sergey Potapov]]></title>
  <link href="http://greyblake.com/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://greyblake.com/"/>
  <updated>2017-08-20T13:58:52+02:00</updated>
  <id>http://greyblake.com/</id>
  <author>
    <name><![CDATA[Sergey Potapov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Rust whatlang library and natural language identification algorithms]]></title>
    <link href="http://greyblake.com/blog/2017/07/30/introduction-to-rust-whatlang-library-and-natural-language-identification-algorithms/"/>
    <updated>2017-07-30T14:03:00+02:00</updated>
    <id>http://greyblake.com/blog/2017/07/30/introduction-to-rust-whatlang-library-and-natural-language-identification-algorithms</id>
    <content type="html"><![CDATA[<p>I'd like to announce a new Rust library <a href="https://github.com/greyblake/whatlang-rs">whatlang</a>.
Its purpose is to detect natural languages by a given text. Let me show you a quick example:</p>

<p>```rust
extern crate whatlang;</p>

<p>use whatlang::detect;</p>

<p>fn main() {</p>

<pre><code>// A sentence in German
let text = "Das ist einfach Deutsch.";

// Detect langauge and unwrap the infromation
let info = detect(&amp;text).unwrap();

// Print an ISO 639-3 language code (e.g. "eng", "rus", "deu", etc)
println!("Detected language: {:?}", info.lang().to_code());

// Print a script (e.g. "Latin", "Cyrillic", "Arabic", etc)
println!("Script: {:?}", info.script());

// Can we rely on this information?
println!("Is reliable: {}", info.is_reliable());
</code></pre>

<p>}
```</p>

<p>The output:</p>

<pre><code>Detected language: deu
Script: Latin
Is reliable: true
</code></pre>

<!--more-->


<p>So, we can see, that library successfully detect short German sentence and we can even trust this information.</p>

<p>You probably noticed, that we had to unwrap info, it's because <code>detect</code> function returns <code>Option&lt;Info&gt;</code>.
It may return <code>None</code>, if the text does not contain any valuable information for language detection
(e.g. numbers or punctuations).</p>

<h2>How does it work?</h2>

<p>There are two steps:</p>

<ul>
<li>Identify a script (writing system)</li>
<li>Identify a language based using trigram language profiles</li>
</ul>


<h3>Script identification</h3>

<p>This  part is very simple. We just iterate over the string by characters, and a script that has the most characters in the text is the winner.
Scripts are presented in UTF-8 as non-overlapping unicode blocks.
You can find information about them in the Internet, here is for example Wikipedia article about <a href="https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)">Basic Latin block</a>.</p>

<h3>Trigram based language detection</h3>

<p>You probably know, that every written language has it's own statistic characteristics.
For example in English text the most used letters are:</p>

<ul>
<li><code>e</code> - 12.7%</li>
<li><code>t</code> - 9%</li>
<li><code>a</code> - 8.2%</li>
</ul>


<p>And so on. So let's say, if we have a big text with frequency of occurrence of "e" equal 12.7%, "t" equal 9%, and "a" equal 8.2%, we may
claim that this text is written in English.</p>

<p>The idea here is similar, but with trigrams.</p>

<h3>What is a trigram?</h3>

<p>A trigram is a particular case of <a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a>, that consists of three items.
Instead of long words, I better give you an example. Let's say we have the following text:</p>

<pre><code>love it
</code></pre>

<p>Trigrams for this text would be: <code>_lo</code>, <code>lov</code>, <code>ove</code>, <code>ve_</code>, <code>e_i</code>, <code>_it</code>, <code>it_</code>.
The underscore character <code>_</code> here just represents the word boundaries.</p>

<h3>Still how does it work?</h3>

<p>The library keeps a list of 300 the most frequent trigrams for every language, sorted by frequency. This is called a language profile.
For an input text, we calculate trigrams and sort them by frequency, and after that we compare this with the known language profiles.
The language, that has the most similar profile to the profile of the input text is the winner.
This idea was presented in the whitepaper <a href="http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf">Cavnar and Trenkle '94: N-Gram-Based Text Categorization'</a> and I it's a must-read
if one wants to understand some algorithmic details.</p>

<h3>How <em>is_reliable</em> calculated?</h3>

<p>It is based on the following factors:</p>

<ul>
<li>How many unique trigrams are in the given text</li>
<li>How big is the difference between the first and the second(not returned) detected languages? This metric is called <code>rate</code> in the code base.</li>
</ul>


<p>Therefore, it can be presented as a 2D space with threshold functions, that splits it into "Reliable" and "Not reliable" areas.
This function is a hyperbola and it looks similar to the following one:</p>

<p><img src="https://raw.githubusercontent.com/greyblake/whatlang-rs/master/misc/images/whatlang_is_reliable.png" alt="Whatlang is reliable" /></p>

<p>Meaning, the more unique N-grams are in the given text (which correlates with the text length), the more chances to get reliable result,
which we can trust.</p>

<h2>Advantages and disadvantages of the trigram approach</h2>

<p>Advantages:</p>

<ul>
<li>Simple</li>
<li>Fast</li>
<li>Memory and CPU efficient</li>
<li>Generic approach that works well for all languages regardless the their grammar</li>
</ul>


<p>Disadvantages:</p>

<ul>
<li>May provide falsy results for short texts (smaller than 200-300 letters). Whatlang tries to compensate this with <code>is_reliable</code> attribute.</li>
</ul>


<h2>Alternative approaches</h2>

<p>One of other approaches I know is to split input text by words, and lookup the words in language dictionaries.
This approach may provide better results for short texts, but it's much more complex in its implementation and slower.
This would require storing all the words in a database or <a href="https://en.wikipedia.org/wiki/Bloom_filter">bloom filters</a>.</p>

<p>For the best ultimate (and pretty complex) solution one may implement a hybrid appoach: use the trigram algorithm for long texts and use
dictionary lookup for short texts.</p>

<h2>Conclusion</h2>

<p>It was a small introduction to <a href="https://github.com/greyblake/whatlang-rs">whatlang</a> library and language identification algorithms.
For information how to use the library (e.g. set a blacklist languages) please check the <a href="https://docs.rs/whatlang">documentation</a>.</p>

<p>The next step will be porting the library to C language.
There is a <a href="https://github.com/greyblake/whatlang-rs/issues/8">ticket</a> for this.
If I manage to do it, I'll write another article.</p>

<p>Thanks for the reading.</p>

<p>P.S. Your feedback is welcome.</p>

<h2>Links</h2>

<ul>
<li><a href="https://github.com/greyblake/whatlang-rs">whatlang (github repo)</a></li>
<li><a href="https://docs.rs/whatlang">whatlang (docs)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)">Basic Latin block</a></li>
<li><a href="https://en.wikipedia.org/wiki/Letter_frequency">Letter Frequency (wikipedia)</a></li>
<li><a href="http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf">Cavnar and Trenkle '94: N-Gram-Based Text Categorization'</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters (wikipedia)</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLP, Toki Pona and Ruby. Part 2: language detector]]></title>
    <link href="http://greyblake.com/blog/2015/09/25/nlp-toki-pona-and-ruby-par2-language-detector/"/>
    <updated>2015-09-25T23:55:00+02:00</updated>
    <id>http://greyblake.com/blog/2015/09/25/nlp-toki-pona-and-ruby-par2-language-detector</id>
    <content type="html"><![CDATA[<p>Previous articles:</p>

<ul>
<li><a href="/blog/2015/09/20/nlp-toki-pona-and-ruby-part1">Part 1: Intro. Tokenizer</a></li>
</ul>


<p>In the first article we created a simple tokenizer, today we're going to create a
language detector to identify Toki Pona text among other texts.</p>

<p>First I want to say that are at least few good libraries for detecting natural languages in ruby:</p>

<ul>
<li><a href="https://github.com/feedbackmine/language_detector">language_detector</a> - detector based on <a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a></li>
<li><a href="https://github.com/peterc/whatlanguage">whatlanguage</a> - detector based on <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a></li>
</ul>


<p>But those are for mainstream: French, English, German... We want Toki Pona!
Also, since we are focused on Toki Pona only, we can get much more precise results.</p>

<!--more-->


<h2>The algorithm</h2>

<p>My initial idea was dead simple: since there are only 125 Toki Pona words, we can
simply check whether a token is a Toki Pona word or something else. Then it's
easy to calculate a density of Toki Pona words in a given phrase and compare it
against some threshold, where 0 &lt; threshold &lt;= 1.</p>

<p>Here is an example of how such algorithm would work with threshold=0.75 and given
phrase <em>"mi moka e kala suli"</em>.</p>

<p>```
Phrase:   mi   moka   e    kala    suli
Weights:  1    0      1    1       1</p>

<p>Sum weight: 1 + 0 + 1 + 1 + 1 = 4
Words count: 5
Density: 4 / 5 = 0.8</p>

<p>0.8 > threshold => true (it's Toki Pona)
```</p>

<p>The phrase has a typo: instead of word <code>moka</code> there should be word <code>moku</code>.
<em>"mi moku e kala suli"</em> means "I am eating a big fish".</p>

<p>I decided to make the algorithm face real data: I picked ~100 random message
from #tokipona IRC channels and split them into three groups:</p>

<ol>
<li>Messages in Toki Pona</li>
<li>Messages in other languages (mostly English)</li>
<li>Mixed messages (half Toki Pona and half English)</li>
</ol>


<p>I wrote a spec, I expected <code>LanguageDetector.toki_pona?(text)</code> to return <code>true</code> for the first group of messages
and <code>false</code> for others. After playing with the value of the threshold, I made it
work almost for all cases.</p>

<p>One of messages looked like this:
<code>
Moku pona xD
</code></p>

<p>It's obvious that it's pure Toki Pona, then what's wrong with it?
The issue was in the <code>Tokenizer</code> that we had implemented in the <a href="/blog/2015/09/20/nlp-toki-pona-and-ruby-part1">previous article</a>.</p>

<p>For this message it returns 4 tokens: <code>["Moku", "pona", "x", "D"]</code>. While
<code>Moku</code> and <code>pona</code> belong to Toki Pona  vocabulary,  <code>x</code> and <code>D</code> don't.</p>

<h2>Updating Tokenizer</h2>

<p>We, as humans, can see that <code>xD</code> actually must be one token, and it's not a regular word, but a smile.
So I had to update <code>Tokenizer</code> to distinguish words and smiles.</p>

<p>That's the point where I had to introduce the difference between <strong>tokens</strong> and <strong>lexemes</strong>.</p>

<p>As the famous <a href="https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools">Dragon Book</a> says about lexemes:</p>

<p><blockquote><p>A lexeme is a sequence of characters in the source program that matches the pattern<br/>for a token and is identified by the lexical analyzer as an instance of that token.</p></blockquote></p>

<p>And about tokens:</p>

<p><blockquote><p>A token is a pair consisting of a token name and an optional attribute value.<br/>The token name is an abstract symbol representing a kind of lexical unit, e.g.,<br/>a particular keyword, or sequence of input characters denoting an identifier.<br/>The token names are the input symbols that the parser processes.</p></blockquote></p>

<p>So I've updated <code>Tokenizer</code> to return array of hashes with <code>:lexeme</code> and <code>:type</code> keys.
By now I have 3 types of tokens: <em>word</em>, <em>smile</em> and <em>punctuation</em>.</p>

<p>Example:
```ruby
Tokenizer.tokenize "moku li ike :("  # Translation: food is bad</p>

<h1>=> [ {:lexeme=>"moku", :type=>:word}, {:lexeme=>"li", :type=>:word},</h1>

<h1>{:lexeme=>"ike", :type=>:word}, {:lexeme=>":/", :type=>:smile}]</h1>

<p>```</p>

<p>The implementation of <code>Tokenizer</code> now is the following:</p>

<p>```ruby
class Tokenizer
  SMILE_REGEXP = /</p>

<pre><code>(?:
  (?: : | ; | = )                            # eyes
  -?                                         # nose
  (?: \) | \| | \\ | \/ | D | P | p | \* )   # mouth
) | (?:x|X)D                                 # other (e.g. XD)
</code></pre>

<p>  /x
  WORD_REGEXP = /\w+/
  PUNCTUATION_REGEX = /[<sup>\s]*/</sup>
  LEXEME_REGEXP = /#{SMILE_REGEXP}|#{WORD_REGEXP}|#{PUNCTUATION_REGEX}/</p>

<p>  def self.tokenize(text)</p>

<pre><code>new(text).tokenize
</code></pre>

<p>  end</p>

<p>  def initialize(text)</p>

<pre><code>@text = text
</code></pre>

<p>  end</p>

<p>  def tokenize</p>

<pre><code>lexemes = @text.scan(LEXEME_REGEXP)
lexemes.map { |lex| lexeme_to_token(lex) }
</code></pre>

<p>  end</p>

<p>  private def lexeme_to_token(lexeme)</p>

<pre><code>{ lexeme: lexeme, type: token_type(lexeme) }
</code></pre>

<p>  end</p>

<p>  private def token_type(lexeme)</p>

<pre><code>case lexeme
when SMILE_REGEXP then :smile
when WORD_REGEXP  then :word
else :punctuation
end
</code></pre>

<p>  end
end
```</p>

<p>Now tokenization process contains two stage: detecting lexemes and defining tokens
for those lexemes. Note, that the order of regular expressions in <code>token_type(lexeme)</code>
method is important. Since <code>SMILE_REGEXP</code> and <code>WORD_REGEXP</code> can overlap (e.g. "XD"),
we want <code>SMILE_REGEXP</code> to have higher priority. The same with punctuation: everything
what is not a smile or a word we consider as punctuation.</p>

<h2>Levenshtein distance</h2>

<p>So I've I updated the language detection algorithm to count only words. Still
there are number of things to improve.</p>

<p>What if some words are meant to be Toki Pona words but they contain a typo?
Actually we can detect them and adjust the algorithm to count them as well.</p>

<p>So to detect a word with a typo we need somehow to calculate word similarity.
And actually what we need is called <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a>.</p>

<p><blockquote><p>Levenshtein distance between two words is the minimum number of single-character<br/>edits (i.e. insertions, deletions or substitutions) required to change one word into the other.</p></blockquote></p>

<p>Levenshtein distance is used in computer science (e.g. for spell checkers), genetics (comparison of gens)
and likely in some other areas. The algorithm isn't the simplest one, but is not hard to understand, so
I encourage you to take a look at <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">wikipedia</a> to
uderstand it better.</p>

<p>Now back to our case: how do we measure the difference between words <code>moka</code> and <code>moku</code>?
The levenshtein distance for them is 1, because only one edit needs to be performed to make <code>moka</code> become
<code>moku</code>: replace <code>a</code> with <code>u</code>.</p>

<p>We can implement it based on the algorithm ourself (actually I did it for fun, but then replaced it
with <a href="https://github.com/threedaymonk/text/blob/master/lib/text/levenshtein.rb">this implementation</a>) or google
for existing solutions.
One of suprises was to find it in
<a href="https://github.com/rubygems/rubygems/blob/45966be372d85520630143090b82b455d287cec6/lib/rubygems/text.rb#L42-L72">rubygems</a> gem.
I guess it's used when one mistypes name of gem in order to have "Did you mean ...?" feature.</p>

<h2>Updating the detection algorithm</h2>

<p>Now, we can adjust the LanguageDetector to be not so strict with typos, and give a word score <code>0.5</code> if it has
levenshtein distance 1 with one of Toki Pona words. Considering the previous example with phrase <em>"mi moka e kala suli"</em>,
the density will be 0.9:</p>

<p>```
Phrase:   mi   moka   e    kala    suli
Weights:  1    0.5    1    1       1</p>

<p>Sum weight: 1 + 0.5 + 1 + 1 + 1 = 4.5
Words count: 5
Density: 4.5 / 5 = 0.9</p>

<p>0.9 > threshold => true (it's Toki Pona)
```</p>

<p>In reality, apart from the mentioned 125 words, the Toki Pona vocablurary includes names of languages, countries and cities.
So the real detector is slightly more complected. You can check it
<a href="https://github.com/greyblake/tokipona/blob/60d8ec72f2da6af26440239e8cb1f0fed5bea8a5/lib/tokipona/language_detector.rb">here</a>.</p>

<p>Btw, the entire implementation of what is being described can be found as the project at
github <a href="https://github.com/greyblake/tokipona">greyblake/tokipona</a>.</p>

<p>That's it for now! In the next part we will try to implement grammar and parser for Toki Pona!</p>

<h2>Links</h2>

<ul>
<li><a href="blog/2015/09/20/nlp-toki-pona-and-ruby-part1">Part 1: implementing Tokenizer</a></li>
<li><a href="http://tokipona.org/">Official Toki Pona site</a></li>
<li><a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance at Wikipedia</a></li>
<li><a href="https://github.com/greyblake/tokipona">My tokipona project at Github</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLP, Toki Pona and Ruby: part 1]]></title>
    <link href="http://greyblake.com/blog/2015/09/20/nlp-toki-pona-and-ruby-part1/"/>
    <updated>2015-09-20T20:36:00+02:00</updated>
    <id>http://greyblake.com/blog/2015/09/20/nlp-toki-pona-and-ruby-part1</id>
    <content type="html"><![CDATA[<h2>Intro</h2>

<p>During last few years, I spent a lot of time learning foreign languages like Esperanto, Spanish and German.
After a while, I came up with an idea that I can apply this knowledge in computer science.</p>

<p>When I decided this I was completely new to Computational Linguistics(CL) and Natural Language Processing(NLP).
However after reading a number of articles I got some basic ideas.</p>

<h2>What I am gonna do</h2>

<p>To dive into CL/NLP I've decided implement Toki Pona -> English translator from scratch.
It's interesting to see which issues I will face and how I will solve them.
It will make me go through number of stages of language processing:</p>

<ul>
<li>Lexical analysis</li>
<li>Language detection (I want to distinguish Toki Pona from other languages)</li>
<li>Morphological analysis (actually will be skipped because of simplicity of Toki Pona)</li>
<li>Syntax analysis</li>
<li>Word translation</li>
<li>Syntax tree conversion</li>
<li>Generation of final translation with respect to English grammar.</li>
</ul>


<p>Anyway, this list is not strict, and probably it will be modified in the future.</p>

<h2>What I am not gonna do</h2>

<p>There are many tools and libraries that already exist in Ruby for NLP.
I am not gonna use any of them here neither cover them in the articles.
If you need something like that, please take a look at <a href="https://github.com/diasks2/ruby-nlp">ruby-nlp</a>.
It's a document that gathers a variety of NLP tools implemented in ruby.</p>

<!--more-->


<h2>What is Toki Pona?</h2>

<p><a href="https://en.wikipedia.org/wiki/Toki_Pona">Toki Pona</a> is a constructed language created by Sonja Lang in 2001.
What is so special about it? Its vocabulary is limited and contains only <strong>125 words</strong>.
The grammar is regular (anyway there will be some pitfalls). The language itself simple and can be learned in 1-2 nights,
and I believe it allows to express 80-90% of daily human communication. Also, it has some philosophical background:
speaking the language you realize what things really are.</p>

<p>Example: there is no word like "friend", one would say "jan pona", what literally  means "good person/human".
In similar way "an ocean" is "telo suli" (big water), "juice" is "telo kili" (water of fruit or vegetable), etc.</p>

<p>So, even Toki Pona is not real <em>natural</em> language, it's good to experiment with, and it gives me some hope that my
goal can be achieved :)</p>

<p>And the end of this article you'll find number of useful links if you want to get into the language.</p>

<h2>First step: lexical analysis</h2>

<p>The first step in processing natural or programming language is <strong>lexical analysis</strong>. It means splitting sequence of
characters into some meaningful units: <strong>tokens</strong>. Sometimes the process is called <strong>tokenization</strong> and
the tools that do it are <strong>tokenizers</strong> or <strong>lexical analyzers</strong>.</p>

<p>Let's see an example. Given a sentence:
<code>
jan suli li pona.
</code>
Translation: "Big man is good"
(<em>jan</em> - human/man, <em>suli</em> - big, <em>li</em> - is/are, <em>pona</em> - good).</p>

<p>Note: in Toki Pona the main word goes first, so noun(<em>jan</em>) is on the first position,
and on the second position is adjective(<em>suli</em>) that modifies the noun.</p>

<p>Expected list of tokens is
<code>ruby
["jan", "suli", "li", "pona", "."]
</code></p>

<p>Let's implement class <code>Tokipona::Tokenizer</code> with a class method <code>.tokenize</code> that returns an array of
tokens for a given text. We start with tests first.</p>

<p>```ruby
describe Tokipona::Tokenizer do
  describe ".tokenize" do</p>

<pre><code>context "only words" do
  it "returns array of words" do
    text = "toki mi li toki pona"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["toki", "mi", "li", "toki", "pona"]
  end
end

context "words with multiple spaces in between" do
  it "returns array of words" do
    text = "toki   mi   li   toki   pona"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["toki", "mi", "li", "toki", "pona"]
  end
end

context "words with special characters" do
  it "returns array of words and characters" do
    text = "sina wile lape anu seme, jan lane?"
    tokens = described_class.tokenize(text)
    expect(tokens).to eq ["sina", "wile", "lape", "anu", "seme", ",", "jan", "lane", "?"]
  end
end

it "does not change input text" do
  text = "toki mi li pona"
  described_class.tokenize(text)
  expect(text).to eq "toki mi li pona"
end
</code></pre>

<p>  end
end
```</p>

<p>Usually lexical analysis for programming languages is based on <a href="http://web.cse.ohio-state.edu/~gurari/course/cse756/html/cse756se2.html">finite-state automata</a>.
But in our simple case we can easily handle it with one regular expression:</p>

<p>```ruby
module Tokipona
  class Tokenizer</p>

<pre><code>def self.tokenize(text)
  @text.scan(/\w+|[^\s]/)
end
</code></pre>

<p>  end
end
```</p>

<p>This implementation looks very naive, but specs pass, so we leave it as it is.
Probably in the future we will modify.</p>

<h2>Conclusion</h2>

<p>It is the first article and the beginning of the journey. The next step will be an implementation
of Toki Pona language detector. It's not necessary to know Toki Pona to follow me,
but in case you are interested, here below I provide some useful links, so you can learn yourself
and start communicating.</p>

<p>I've created a github repository where you can access the code: <a href="https://github.com/greyblake/tokipona">greyblake/tokipona</a>.</p>

<p>P.S.</p>

<p>Thanks for reading. The subject is new for me, so your comments, suggestions and feedback can be very helpful.</p>

<h2>Links</h2>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Toki_Pona">Toki Pona at Wikipedia</a></li>
<li><a href="http://tokipona.org/">Official Toki Pona site</a></li>
<li><a href="http://rowa.giso.de/languages/toki-pona/english/lessons.php">Toki Pona lessons</a> - here you can start learning the language</li>
<li><a href="http://x-raizor.github.io/visual-tokipona/index.html">Toki Pona visual vocabulary</a></li>
<li>#tokipona - IRC channel where you can communicate with other people</li>
</ul>

]]></content>
  </entry>
  
</feed>
